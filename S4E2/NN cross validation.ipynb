{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the data\n",
    "train=pd.read_csv('train.csv')\n",
    "test=pd.read_csv('test.csv')\n",
    "original=pd.read_csv('ObesityDataSet.csv')\n",
    "submission=pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.concat([train,original],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Gender',\n",
       " 'family_history_with_overweight',\n",
       " 'FAVC',\n",
       " 'CAEC',\n",
       " 'SMOKE',\n",
       " 'SCC',\n",
       " 'CALC',\n",
       " 'MTRANS',\n",
       " 'NObeyesdad']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get categorical columns\n",
    "categorical_columns=[]\n",
    "for col in train.columns:\n",
    "    if train[col].dtype=='object':\n",
    "        categorical_columns.append(col)\n",
    "\n",
    "categorical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical column encoding; Here I use manual encoding\n",
    "train['Gender']=train[\"Gender\"].apply(lambda x: 1 if x==\"Male\" else 0)\n",
    "test['Gender']=test[\"Gender\"].apply(lambda x: 1 if x==\"Male\" else 0)\n",
    "\n",
    "train['family_history_with_overweight']=train[\"family_history_with_overweight\"].apply(lambda x: 1 if x==\"yes\" else 0)\n",
    "test['family_history_with_overweight']=test[\"family_history_with_overweight\"].apply(lambda x: 1 if x==\"yes\" else 0)\n",
    "\n",
    "train['FAVC']=train[\"FAVC\"].apply(lambda x: 1 if x==\"yes\" else 0)\n",
    "test['FAVC']=test[\"FAVC\"].apply(lambda x: 1 if x==\"yes\" else 0)\n",
    "\n",
    "train['CAEC']=train[\"CAEC\"].apply(lambda x: 1 if x==\"no\" else 2 if x==\"Sometimes\" else 3 if x==\"Always\" else 4)\n",
    "test['CAEC']=test[\"CAEC\"].apply(lambda x: 1 if x==\"no\" else 2 if x==\"Sometimes\" else 3 if x==\"Always\" else 4)\n",
    "\n",
    "train['SMOKE']=train[\"SMOKE\"].apply(lambda x: 1 if x==\"yes\" else 0)\n",
    "test['SMOKE']=test[\"SMOKE\"].apply(lambda x: 1 if x==\"yes\" else 0)\n",
    "\n",
    "train['SCC']=train[\"SCC\"].apply(lambda x: 1 if x==\"yes\" else 0)\n",
    "test['SCC']=test[\"SCC\"].apply(lambda x: 1 if x==\"yes\" else 0)\n",
    "\n",
    "train['CALC']=train[\"CALC\"].apply(lambda x: 1 if x==\"no\" else 2 if x==\"Sometimes\" else 3)\n",
    "test['CALC']=test[\"CALC\"].apply(lambda x: 1 if x==\"no\" else 2 if x==\"Sometimes\" else 3)\n",
    "\n",
    "mapping={'Public_Transportation':1,\n",
    "         'Automobile':2,\n",
    "         'Motorbike':3,\n",
    "         'Bike':4,\n",
    "         'Walking':5}\n",
    "train['MTRANS']=train[\"MTRANS\"].replace(mapping)\n",
    "test[\"MTRANS\"]=test[\"MTRANS\"].replace(mapping)\n",
    "\n",
    "# target label encoding\n",
    "target_mapping={'Insufficient_Weight':0,\n",
    "                'Normal_Weight':1,\n",
    "                'Overweight_Level_I':2,\n",
    "                'Overweight_Level_II':3,\n",
    "                'Obesity_Type_I':4,\n",
    "                'Obesity_Type_II':5,\n",
    "                'Obesity_Type_III':6}\n",
    "\n",
    "train['NObeyesdad']=train[\"NObeyesdad\"].replace(target_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "\n",
    "def feat_eng(df):\n",
    "    df['BMI'] = df['Weight'] / (df['Height']**2)\n",
    "    df[\"HealthyHabitRatio\"] = (df[\"FCVC\"] + df[\"CH2O\"] + df[\"FAF\"]) / (df[\"FAVC\"] + df[\"CAEC\"] + df[\"TUE\"] + df[\"SMOKE\"] * 2)\n",
    "    df[\"Age_BMI\"] = df[\"Age\"] * df[\"BMI\"]\n",
    "    df[\"Age_HealthyHabitRatio\"] = df[\"Age\"] * df[\"HealthyHabitRatio\"]\n",
    "    df[\"Gender_SCC\"]=df[\"Gender\"]*df[\"SCC\"]\n",
    "    df[\"Height_Weight_Ratio\"]=df[\"Height\"]/df[\"Weight\"]\n",
    "    df[\"FAVC_CAEC_Index\"]=df[\"FAVC\"]/df[\"CAEC\"]\n",
    "    df[\"Activity_Index\"]=df[\"FAF\"]-df[\"TUE\"]\n",
    "    df[\"Water_Alcohol_Ratio\"]=df[\"CH2O\"]/df[\"CALC\"]\n",
    "    df[\"Meal_Frequency_Deviation\"]=abs(df[\"NCP\"]-3+1e-6)\n",
    "    df[\"FamilyHistory_BMI_Interaction\"]=(df[\"family_history_with_overweight\"]+1e-6)*df[\"BMI\"]\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=feat_eng(train)\n",
    "test=feat_eng(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler=StandardScaler()\n",
    "\n",
    "test.drop(columns=[\"id\"],axis=1,inplace=True)\n",
    "X= train.drop(columns=[\"NObeyesdad\",\"id\"],axis=1)\n",
    "y=train[\"NObeyesdad\"]\n",
    "X=scaler.fit_transform(X) \n",
    "test=scaler.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_50394/3748468421.py:19: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n",
      "  plt.style.use('seaborn-whitegrid')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.functional as F\n",
    "import torch.optim.lr_scheduler as lr_sheduler\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix,roc_curve, precision_recall_curve, auc\n",
    "from tqdm.notebook import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "class DNN(object):\n",
    "    def __init__(self,model,loss_fn,optimizer,es_patience):\n",
    "        # arguments as attributes\n",
    "        self.model=model\n",
    "        self.loss_fn=loss_fn\n",
    "        self.optimizer=optimizer\n",
    "        self.device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        # attributes to use in future, currently empty\n",
    "        self.train_loader=None\n",
    "        self.val_loader=None\n",
    "        self.writer=None\n",
    "\n",
    "        self.patience=es_patience\n",
    "        self.min_delta=0\n",
    "        self.counter=0\n",
    "        self.min_validation_loss=float('inf')\n",
    "\n",
    "        # attributes to be computed internally\n",
    "        self.losses=[]\n",
    "        self.val_losses=[]\n",
    "        self.total_epochs=0\n",
    "\n",
    "        # train step function\n",
    "        self.train_step_fn=self._make_train_step_fn()\n",
    "        # validation step function\n",
    "        self.val_step_fn=self._make_val_step_fn()\n",
    "\n",
    "    def to(self,device):\n",
    "        try:\n",
    "            self.device=device\n",
    "            self.model.to(self.device)\n",
    "        except RuntimeError:\n",
    "            self.device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "            print(f\"Can't move to {device}, moving to {self.device} instead\")\n",
    "            self.model.to(self.device)\n",
    "\n",
    "    def set_loaders(self,train_loader,val_loader=None):\n",
    "        self.train_loader=train_loader\n",
    "        self.val_loader=val_loader\n",
    "\n",
    "    def set_tensorboard(self,name,folder='runs'):\n",
    "        suffix=datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        self.writer=SummaryWriter(f\"{folder}/{name}_{suffix}\")\n",
    "\n",
    "    def _make_train_step_fn(self):\n",
    "        def peform_train_step_fn(X,y):\n",
    "            # set model to train mode\n",
    "            self.model.train()\n",
    "            # forward pass\n",
    "            yhat=self.model(X)\n",
    "            # compute the loss\n",
    "            loss=self.loss_fn(yhat,y.squeeze())\n",
    "            # compute gradients  \n",
    "            loss.backward()\n",
    "            # update weights\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            return loss.item()\n",
    "        return peform_train_step_fn \n",
    "\n",
    "    def _make_val_step_fn(self):\n",
    "        def perform_val_step_fn(X,y):\n",
    "            # set model to eval mode\n",
    "            self.model.eval()\n",
    "\n",
    "            yhat=self.model(X) \n",
    "            loss=self.loss_fn(yhat,y)\n",
    "            return loss.item()\n",
    "        return perform_val_step_fn\n",
    "    \n",
    "\n",
    "    def _should_early_stop(self,val_loss):\n",
    "        if val_loss<self.min_validation_loss:\n",
    "            self.min_validation_loss=val_loss\n",
    "            self.counter=0\n",
    "        elif val_loss>self.min_validation_loss:\n",
    "            self.counter+=1\n",
    "            if self.counter>=self.patience:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    \n",
    "    def _mini_batch(self,validation=False):\n",
    "        if validation:\n",
    "            data_loader=self.val_loader\n",
    "            step_fn=self.val_step_fn\n",
    "        else:\n",
    "            data_loader=self.train_loader\n",
    "            step_fn=self.train_step_fn\n",
    "        if data_loader is None:\n",
    "            return None\n",
    "        \n",
    "        mini_batch_losses=[]\n",
    "        for X_batch,y_batch in data_loader:\n",
    "            X_batch=X_batch.to(self.device)\n",
    "            y_batch=y_batch.to(self.device)\n",
    "\n",
    "            mini_batch_loss=step_fn(X_batch,y_batch)\n",
    "            mini_batch_losses.append(mini_batch_loss)\n",
    "\n",
    "        loss=np.mean(mini_batch_losses)\n",
    "        return loss\n",
    "    \n",
    "    def set_scheduler(self,patience=5,mode=\"min\",factor=0.1,min_lr=1e-6):\n",
    "        self.scheduler=lr_sheduler.ReduceLROnPlateau(self.optimizer,\n",
    "                                                     patience=patience,\n",
    "                                                     mode=mode,\n",
    "                                                     factor=factor,\n",
    "                                                     min_lr=min_lr)\n",
    "    \n",
    "    def set_seed(self,seed=42):\n",
    "        torch.backends.cudnn.deterministic=True\n",
    "        torch.backends.cudnn.benchmark=False\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    def train(self,n_epochs,seed=42):\n",
    "        self.set_seed(seed)\n",
    "\n",
    "        for epoch in tqdm(range(n_epochs)):\n",
    "            self.total_epochs+=1\n",
    "\n",
    "            # inner loop, perform training using mini batches\n",
    "            loss=self._mini_batch(validation=False) # validation=False because we are training\n",
    "            self.losses.append(loss)\n",
    "\n",
    "            # validation\n",
    "            with torch.no_grad():\n",
    "                val_loss=self._mini_batch(validation=True)\n",
    "                self.val_losses.append(val_loss)\n",
    "                # Update learning rate sheduler\n",
    "                self.scheduler.step(val_loss) \n",
    "                # Early stopping\n",
    "                if self._should_early_stop(val_loss):\n",
    "                    print(\"Early Stopping\")\n",
    "                    break\n",
    "            \n",
    "\n",
    "            # for summary writer\n",
    "            if self.writer:\n",
    "                scalars={'training':loss}\n",
    "                if val_loss is not None:\n",
    "                    scalars.update({'validation':val_loss})\n",
    "                self.writer.add_scalars(main_tag='loss',tag_scalar_dict=scalars,global_step=epoch) # global step is the x-axis\n",
    "\n",
    "        if self.writer:\n",
    "            self.writer.close() # close the writer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    def save_checkpoint(self,filename):\n",
    "        # Build a dictionary with all information for resume training\n",
    "        checkpoint={'epoch':self.total_epochs,\n",
    "                    'model_state_dict':self.model.state_dict(),\n",
    "                    'optimizer_state_dict':self.optimizer.state_dict(),\n",
    "                    'loss':self.losses,\n",
    "                    'val_loss':self.val_losses}\n",
    "        torch.save(checkpoint,filename)\n",
    "\n",
    "    def load_checkpoint(self,filename):\n",
    "        checkpoint=torch.load(filename)\n",
    "\n",
    "        # Restore the state of the model\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        self.total_epochs=checkpoint['epoch']\n",
    "        self.losses=checkpoint['loss']\n",
    "        self.val_losses=checkpoint['val_loss']\n",
    "\n",
    "        self.model.train() # always set the model to training mode after loading the checkpoint\n",
    "        \n",
    "    def predict(self,X):\n",
    "        self.model.eval()\n",
    "        X_tensor=torch.as_tensor(X,dtype=torch.float32,device=self.device)\n",
    "        yhat_tensor=self.model(X_tensor)\n",
    "        self.model.train()\n",
    "        return yhat_tensor.detach().cpu().numpy()\n",
    "    \n",
    "    def plot_losses(self):\n",
    "        fig=plt.figure(figsize=(10,4))\n",
    "        plt.plot(self.losses,label='Training Loss',c='b')\n",
    "        plt.plot(self.val_losses,label='Validation Loss',c='r')\n",
    "        plt.yscale('log')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel(\"loss\")\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        return fig\n",
    "    \n",
    "    def add_graph(self):\n",
    "        # fetches a single mini batch\n",
    "        if self.train_loader and self.writer:\n",
    "            X_sample,y_sample=next(iter(self.train_loader))\n",
    "            self.writer.add_graph(self.model,X_sample.to(self.device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0c3ffa230644efe9fc4f2b13d4b2c00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2f2169e38ec4dc89824982b0361d645",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4965d0c0eab4becbd728b7613493892",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71e159db46494f6392882cb873fb3a5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e1ddddc7cc4406baf8b3ac0c705caf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping\n",
      "[0.8891560996939222, 0.890249234805422, 0.8965894184521207, 0.8858766943594228, 0.8921933085501859]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, RepeatedStratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "skf=StratifiedKFold(n_splits=5,shuffle=True,random_state=42)\n",
    "all_preds=[]\n",
    "for train_idx, val_idx in skf.split(X,y):\n",
    "\n",
    "    # Data preprocessing\n",
    "    X_train,X_val=X[train_idx],X[val_idx]\n",
    "    y_train,y_val=y.iloc[train_idx],y.iloc[val_idx]\n",
    "\n",
    "    # convert pd series objectd to numpy arrays\n",
    "    y_train=y_train.to_numpy()\n",
    "    y_val=y_val.to_numpy()\n",
    "\n",
    "    torch.manual_seed(13)\n",
    "    X_train_tensor=torch.as_tensor(X_train).float()\n",
    "    y_train_tensor=torch.as_tensor(y_train).long()\n",
    "\n",
    "    X_val_tensor=torch.as_tensor(X_val).float()\n",
    "    y_val_tensor=torch.as_tensor(y_val).long()\n",
    "\n",
    "    train_dataset=TensorDataset(X_train_tensor,y_train_tensor)\n",
    "    val_dataset=TensorDataset(X_val_tensor,y_val_tensor)\n",
    "\n",
    "    train_loader=DataLoader(train_dataset,batch_size=128,shuffle=True)\n",
    "    val_loader=DataLoader(val_dataset,batch_size=128)\n",
    "        \n",
    "    \n",
    "    # Model config\n",
    "    learning_rate=0.01\n",
    "\n",
    "    torch.manual_seed(42)\n",
    "    model=nn.Sequential()\n",
    "    model.add_module('hidden1',nn.Linear(27,88))\n",
    "    model.add_module('activation1',nn.ReLU())\n",
    "    model.add_module('batchNorm1',nn.BatchNorm1d(88))\n",
    "    model.add_module('dropout1',nn.Dropout(p=0.2))\n",
    "\n",
    "    model.add_module('hidden2',nn.Linear(88,36))\n",
    "    model.add_module('activation2',nn.ReLU())\n",
    "    model.add_module('batchNorm2',nn.BatchNorm1d(36))\n",
    "    model.add_module('dropout2',nn.Dropout(p=0.2))\n",
    "\n",
    "    model.add_module('hidden4',nn.Linear(36,18))\n",
    "    model.add_module('activation4',nn.ReLU())\n",
    "    model.add_module('batchNorm4',nn.BatchNorm1d(18))\n",
    "    model.add_module('dropout4',nn.Dropout(p=0.2))\n",
    "\n",
    "    model.add_module('output',nn.Linear(18,7))\n",
    "    model.add_module('softmax',nn.Softmax(dim=1))\n",
    "\n",
    "    optimizer=optim.Adam(model.parameters(),lr=learning_rate)\n",
    "    loss_fn=nn.CrossEntropyLoss() \n",
    "\n",
    "    es_patience=20\n",
    "\n",
    "    # Model Training\n",
    "    n_epochs=200\n",
    "    ann=DNN(model,loss_fn,optimizer,es_patience)\n",
    "    ann.set_loaders(train_loader,val_loader)\n",
    "    ann.set_scheduler(patience=5,factor=0.3) #ann.set_scheduler(patience=5,factor=0.4)\n",
    "    ann.train(n_epochs)\n",
    "    \n",
    "    all_preds.append(accuracy_score(y_val,ann.predict(X_val).argmax(axis=1)))\n",
    "\n",
    "print(all_preds)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8908129511722148"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(all_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best epoch: 208\n",
      "Custom score best: 1082.52\n",
      "Custom score current: 801.08\n",
      "Min_val_loss: 1.2792\n"
     ]
    }
   ],
   "source": [
    "# # custom score id is defined by me for experimenting\n",
    "# print(f\"Best epoch: {ann.val_losses.index(np.min(ann.val_losses))}\\nCustom score best: {np.round((1.29-min(ann.val_losses))*1e5,2)}\\nCustom score current: {np.round((1.29-ann.val_losses[-1])*1e5,2)}\\nMin_val_loss: {np.round(min(ann.val_losses),4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
