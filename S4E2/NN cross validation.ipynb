{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the data\n",
    "train=pd.read_csv('train.csv')\n",
    "test=pd.read_csv('test.csv')\n",
    "original=pd.read_csv('ObesityDataSet.csv')\n",
    "submission=pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.concat([train,original],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Gender',\n",
       " 'family_history_with_overweight',\n",
       " 'FAVC',\n",
       " 'CAEC',\n",
       " 'SMOKE',\n",
       " 'SCC',\n",
       " 'CALC',\n",
       " 'MTRANS',\n",
       " 'NObeyesdad']"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get categorical columns\n",
    "categorical_columns=[]\n",
    "for col in train.columns:\n",
    "    if train[col].dtype=='object':\n",
    "        categorical_columns.append(col)\n",
    "\n",
    "categorical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical column encoding; Here I use manual encoding\n",
    "train['Gender']=train[\"Gender\"].apply(lambda x: 1 if x==\"Male\" else 0)\n",
    "test['Gender']=test[\"Gender\"].apply(lambda x: 1 if x==\"Male\" else 0)\n",
    "\n",
    "train['family_history_with_overweight']=train[\"family_history_with_overweight\"].apply(lambda x: 1 if x==\"yes\" else 0)\n",
    "test['family_history_with_overweight']=test[\"family_history_with_overweight\"].apply(lambda x: 1 if x==\"yes\" else 0)\n",
    "\n",
    "train['FAVC']=train[\"FAVC\"].apply(lambda x: 1 if x==\"yes\" else 0)\n",
    "test['FAVC']=test[\"FAVC\"].apply(lambda x: 1 if x==\"yes\" else 0)\n",
    "\n",
    "train['CAEC']=train[\"CAEC\"].apply(lambda x: 1 if x==\"no\" else 2 if x==\"Sometimes\" else 3 if x==\"Always\" else 4)\n",
    "test['CAEC']=test[\"CAEC\"].apply(lambda x: 1 if x==\"no\" else 2 if x==\"Sometimes\" else 3 if x==\"Always\" else 4)\n",
    "\n",
    "train['SMOKE']=train[\"SMOKE\"].apply(lambda x: 1 if x==\"yes\" else 0)\n",
    "test['SMOKE']=test[\"SMOKE\"].apply(lambda x: 1 if x==\"yes\" else 0)\n",
    "\n",
    "train['SCC']=train[\"SCC\"].apply(lambda x: 1 if x==\"yes\" else 0)\n",
    "test['SCC']=test[\"SCC\"].apply(lambda x: 1 if x==\"yes\" else 0)\n",
    "\n",
    "train['CALC']=train[\"CALC\"].apply(lambda x: 1 if x==\"no\" else 2 if x==\"Sometimes\" else 3)\n",
    "test['CALC']=test[\"CALC\"].apply(lambda x: 1 if x==\"no\" else 2 if x==\"Sometimes\" else 3)\n",
    "\n",
    "mapping={'Public_Transportation':1,\n",
    "         'Automobile':2,\n",
    "         'Motorbike':3,\n",
    "         'Bike':4,\n",
    "         'Walking':5}\n",
    "train['MTRANS']=train[\"MTRANS\"].replace(mapping)\n",
    "test[\"MTRANS\"]=test[\"MTRANS\"].replace(mapping)\n",
    "\n",
    "# target label encoding\n",
    "target_mapping={'Insufficient_Weight':0,\n",
    "                'Normal_Weight':1,\n",
    "                'Overweight_Level_I':2,\n",
    "                'Overweight_Level_II':3,\n",
    "                'Obesity_Type_I':4,\n",
    "                'Obesity_Type_II':5,\n",
    "                'Obesity_Type_III':6}\n",
    "\n",
    "train['NObeyesdad']=train[\"NObeyesdad\"].replace(target_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "\n",
    "def feat_eng(df):\n",
    "    df['BMI'] = df['Weight'] / (df['Height']**2)\n",
    "    df[\"HealthyHabitRatio\"] = (df[\"FCVC\"] + df[\"CH2O\"] + df[\"FAF\"]) / (df[\"FAVC\"] + df[\"CAEC\"] + df[\"TUE\"] + df[\"SMOKE\"] * 2)\n",
    "    df[\"Age_BMI\"] = df[\"Age\"] * df[\"BMI\"]\n",
    "    df[\"Age_HealthyHabitRatio\"] = df[\"Age\"] * df[\"HealthyHabitRatio\"]\n",
    "    df[\"Gender_SCC\"]=df[\"Gender\"]*df[\"SCC\"]\n",
    "    df[\"Height_Weight_Ratio\"]=df[\"Height\"]/df[\"Weight\"]\n",
    "    df[\"FAVC_CAEC_Index\"]=df[\"FAVC\"]/df[\"CAEC\"]\n",
    "    df[\"Activity_Index\"]=df[\"FAF\"]-df[\"TUE\"]\n",
    "    df[\"Water_Alcohol_Ratio\"]=df[\"CH2O\"]/df[\"CALC\"]\n",
    "    df[\"Meal_Frequency_Deviation\"]=abs(df[\"NCP\"]-3+1e-6)\n",
    "    df[\"FamilyHistory_BMI_Interaction\"]=(df[\"family_history_with_overweight\"]+1e-6)*df[\"BMI\"]\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=feat_eng(train)\n",
    "test=feat_eng(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler=StandardScaler()\n",
    "\n",
    "test.drop(columns=[\"id\"],axis=1,inplace=True)\n",
    "X= train.drop(columns=[\"NObeyesdad\",\"id\"],axis=1)\n",
    "y=train[\"NObeyesdad\"]\n",
    "X=scaler.fit_transform(X) \n",
    "test=scaler.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_50394/3748468421.py:19: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n",
      "  plt.style.use('seaborn-whitegrid')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.functional as F\n",
    "import torch.optim.lr_scheduler as lr_sheduler\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix,roc_curve, precision_recall_curve, auc\n",
    "from tqdm.notebook import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "class DNN(object):\n",
    "    def __init__(self,model,loss_fn,optimizer,es_patience):\n",
    "        # arguments as attributes\n",
    "        self.model=model\n",
    "        self.loss_fn=loss_fn\n",
    "        self.optimizer=optimizer\n",
    "        self.device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        # attributes to use in future, currently empty\n",
    "        self.train_loader=None\n",
    "        self.val_loader=None\n",
    "        self.writer=None\n",
    "\n",
    "        self.patience=es_patience\n",
    "        self.min_delta=0\n",
    "        self.counter=0\n",
    "        self.min_validation_loss=float('inf')\n",
    "\n",
    "        # attributes to be computed internally\n",
    "        self.losses=[]\n",
    "        self.val_losses=[]\n",
    "        self.total_epochs=0\n",
    "\n",
    "        # train step function\n",
    "        self.train_step_fn=self._make_train_step_fn()\n",
    "        # validation step function\n",
    "        self.val_step_fn=self._make_val_step_fn()\n",
    "\n",
    "    def to(self,device):\n",
    "        try:\n",
    "            self.device=device\n",
    "            self.model.to(self.device)\n",
    "        except RuntimeError:\n",
    "            self.device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "            print(f\"Can't move to {device}, moving to {self.device} instead\")\n",
    "            self.model.to(self.device)\n",
    "\n",
    "    def set_loaders(self,train_loader,val_loader=None):\n",
    "        self.train_loader=train_loader\n",
    "        self.val_loader=val_loader\n",
    "\n",
    "    def set_tensorboard(self,name,folder='runs'):\n",
    "        suffix=datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        self.writer=SummaryWriter(f\"{folder}/{name}_{suffix}\")\n",
    "\n",
    "    def _make_train_step_fn(self):\n",
    "        def peform_train_step_fn(X,y):\n",
    "            # set model to train mode\n",
    "            self.model.train()\n",
    "            # forward pass\n",
    "            yhat=self.model(X)\n",
    "            # compute the loss\n",
    "            loss=self.loss_fn(yhat,y.squeeze())\n",
    "            # compute gradients  \n",
    "            loss.backward()\n",
    "            # update weights\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            return loss.item()\n",
    "        return peform_train_step_fn \n",
    "\n",
    "    def _make_val_step_fn(self):\n",
    "        def perform_val_step_fn(X,y):\n",
    "            # set model to eval mode\n",
    "            self.model.eval()\n",
    "\n",
    "            yhat=self.model(X) \n",
    "            loss=self.loss_fn(yhat,y)\n",
    "            return loss.item()\n",
    "        return perform_val_step_fn\n",
    "    \n",
    "\n",
    "    def _should_early_stop(self,val_loss):\n",
    "        if val_loss<self.min_validation_loss:\n",
    "            self.min_validation_loss=val_loss\n",
    "            self.counter=0\n",
    "        elif val_loss>self.min_validation_loss:\n",
    "            self.counter+=1\n",
    "            if self.counter>=self.patience:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    \n",
    "    def _mini_batch(self,validation=False):\n",
    "        if validation:\n",
    "            data_loader=self.val_loader\n",
    "            step_fn=self.val_step_fn\n",
    "        else:\n",
    "            data_loader=self.train_loader\n",
    "            step_fn=self.train_step_fn\n",
    "        if data_loader is None:\n",
    "            return None\n",
    "        \n",
    "        mini_batch_losses=[]\n",
    "        for X_batch,y_batch in data_loader:\n",
    "            X_batch=X_batch.to(self.device)\n",
    "            y_batch=y_batch.to(self.device)\n",
    "\n",
    "            mini_batch_loss=step_fn(X_batch,y_batch)\n",
    "            mini_batch_losses.append(mini_batch_loss)\n",
    "\n",
    "        loss=np.mean(mini_batch_losses)\n",
    "        return loss\n",
    "    \n",
    "    def set_scheduler(self,patience=5,mode=\"min\",factor=0.1,min_lr=1e-6):\n",
    "        self.scheduler=lr_sheduler.ReduceLROnPlateau(self.optimizer,\n",
    "                                                     patience=patience,\n",
    "                                                     mode=mode,\n",
    "                                                     factor=factor,\n",
    "                                                     min_lr=min_lr)\n",
    "    \n",
    "    def set_seed(self,seed=42):\n",
    "        torch.backends.cudnn.deterministic=True\n",
    "        torch.backends.cudnn.benchmark=False\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    def train(self,n_epochs,seed=42):\n",
    "        self.set_seed(seed)\n",
    "\n",
    "        for epoch in tqdm(range(n_epochs)):\n",
    "            self.total_epochs+=1\n",
    "\n",
    "            # inner loop, perform training using mini batches\n",
    "            loss=self._mini_batch(validation=False) # validation=False because we are training\n",
    "            self.losses.append(loss)\n",
    "\n",
    "            # validation\n",
    "            with torch.no_grad():\n",
    "                val_loss=self._mini_batch(validation=True)\n",
    "                self.val_losses.append(val_loss)\n",
    "                # Update learning rate sheduler\n",
    "                self.scheduler.step(val_loss) \n",
    "                # Early stopping\n",
    "                if self._should_early_stop(val_loss):\n",
    "                    print(\"Early Stopping\")\n",
    "                    break\n",
    "            \n",
    "\n",
    "            # for summary writer\n",
    "            if self.writer:\n",
    "                scalars={'training':loss}\n",
    "                if val_loss is not None:\n",
    "                    scalars.update({'validation':val_loss})\n",
    "                self.writer.add_scalars(main_tag='loss',tag_scalar_dict=scalars,global_step=epoch) # global step is the x-axis\n",
    "\n",
    "        if self.writer:\n",
    "            self.writer.close() # close the writer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    def save_checkpoint(self,filename):\n",
    "        # Build a dictionary with all information for resume training\n",
    "        checkpoint={'epoch':self.total_epochs,\n",
    "                    'model_state_dict':self.model.state_dict(),\n",
    "                    'optimizer_state_dict':self.optimizer.state_dict(),\n",
    "                    'loss':self.losses,\n",
    "                    'val_loss':self.val_losses}\n",
    "        torch.save(checkpoint,filename)\n",
    "\n",
    "    def load_checkpoint(self,filename):\n",
    "        checkpoint=torch.load(filename)\n",
    "\n",
    "        # Restore the state of the model\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        self.total_epochs=checkpoint['epoch']\n",
    "        self.losses=checkpoint['loss']\n",
    "        self.val_losses=checkpoint['val_loss']\n",
    "\n",
    "        self.model.train() # always set the model to training mode after loading the checkpoint\n",
    "        \n",
    "    def predict(self,X):\n",
    "        self.model.eval()\n",
    "        X_tensor=torch.as_tensor(X,dtype=torch.float32,device=self.device)\n",
    "        yhat_tensor=self.model(X_tensor)\n",
    "        self.model.train()\n",
    "        return yhat_tensor.detach().cpu().numpy()\n",
    "    \n",
    "    def plot_losses(self):\n",
    "        fig=plt.figure(figsize=(10,4))\n",
    "        plt.plot(self.losses,label='Training Loss',c='b')\n",
    "        plt.plot(self.val_losses,label='Validation Loss',c='r')\n",
    "        plt.yscale('log')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel(\"loss\")\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        return fig\n",
    "    \n",
    "    def add_graph(self):\n",
    "        # fetches a single mini batch\n",
    "        if self.train_loader and self.writer:\n",
    "            X_sample,y_sample=next(iter(self.train_loader))\n",
    "            self.writer.add_graph(self.model,X_sample.to(self.device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mode(arr):\n",
    "    return max(set(arr), key=arr.count)\n",
    "\n",
    "def get_mode_by_position(list_of_arrays):\n",
    "    # Use zip to group elements at the same positions\n",
    "    grouped_elements = zip(*list_of_arrays)\n",
    "    # Apply get_mode to each group of elements\n",
    "    mode_array = [get_mode(group) for group in grouped_elements]\n",
    "    return mode_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86907330bea94b26a3d5a34af89ff07c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[152], line 65\u001b[0m\n\u001b[1;32m     63\u001b[0m ann\u001b[38;5;241m.\u001b[39mset_loaders(train_loader,val_loader)\n\u001b[1;32m     64\u001b[0m ann\u001b[38;5;241m.\u001b[39mset_scheduler(patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m) \u001b[38;5;66;03m#ann.set_scheduler(patience=5,factor=0.4)\u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m \u001b[43mann\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''---------------------------------------------------------------------------------------------'''\u001b[39;00m\n\u001b[1;32m     69\u001b[0m y_pred_test\u001b[38;5;241m=\u001b[39mann\u001b[38;5;241m.\u001b[39mpredict(test)\u001b[38;5;241m.\u001b[39margmax(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[150], line 147\u001b[0m, in \u001b[0;36mDNN.train\u001b[0;34m(self, n_epochs, seed)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal_epochs\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;66;03m# inner loop, perform training using mini batches\u001b[39;00m\n\u001b[0;32m--> 147\u001b[0m loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mini_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalidation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# validation=False because we are training\u001b[39;00m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mappend(loss)\n\u001b[1;32m    150\u001b[0m \u001b[38;5;66;03m# validation\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[150], line 121\u001b[0m, in \u001b[0;36mDNN._mini_batch\u001b[0;34m(self, validation)\u001b[0m\n\u001b[1;32m    118\u001b[0m     X_batch\u001b[38;5;241m=\u001b[39mX_batch\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    119\u001b[0m     y_batch\u001b[38;5;241m=\u001b[39my_batch\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m--> 121\u001b[0m     mini_batch_loss\u001b[38;5;241m=\u001b[39m\u001b[43mstep_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m     mini_batch_losses\u001b[38;5;241m.\u001b[39mappend(mini_batch_loss)\n\u001b[1;32m    124\u001b[0m loss\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mmean(mini_batch_losses)\n",
      "Cell \u001b[0;32mIn[150], line 70\u001b[0m, in \u001b[0;36mDNN._make_train_step_fn.<locals>.peform_train_step_fn\u001b[0;34m(X, y)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpeform_train_step_fn\u001b[39m(X,y):\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# set model to train mode\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;66;03m# forward pass\u001b[39;00m\n\u001b[1;32m     72\u001b[0m     yhat\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(X)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py:2394\u001b[0m, in \u001b[0;36mModule.train\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m   2392\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m mode\n\u001b[1;32m   2393\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m-> 2394\u001b[0m     \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py:2392\u001b[0m, in \u001b[0;36mModule.train\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m   2390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mode, \u001b[38;5;28mbool\u001b[39m):\n\u001b[1;32m   2391\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining mode is expected to be boolean\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2392\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m \u001b[38;5;241m=\u001b[39m mode\n\u001b[1;32m   2393\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m   2394\u001b[0m     module\u001b[38;5;241m.\u001b[39mtrain(mode)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py:1735\u001b[0m, in \u001b[0;36mModule.__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   1733\u001b[0m     modules[name] \u001b[38;5;241m=\u001b[39m value\n\u001b[1;32m   1734\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1735\u001b[0m     buffers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__dict__\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_buffers\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1736\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m buffers:\n\u001b[1;32m   1737\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, torch\u001b[38;5;241m.\u001b[39mTensor):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, RepeatedStratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "skf=StratifiedKFold(n_splits=10,shuffle=True,random_state=42)\n",
    "total_losses=[]\n",
    "all_preds=[]\n",
    "acc_scores=[]\n",
    "for train_idx, val_idx in skf.split(X,y):\n",
    "\n",
    "    # Data preprocessing\n",
    "    X_train,X_val=X[train_idx],X[val_idx]\n",
    "    y_train,y_val=y.iloc[train_idx],y.iloc[val_idx]\n",
    "\n",
    "    # convert pd series objectd to numpy arrays\n",
    "    y_train=y_train.to_numpy()\n",
    "    y_val=y_val.to_numpy()\n",
    "\n",
    "    torch.manual_seed(13)\n",
    "    X_train_tensor=torch.as_tensor(X_train).float()\n",
    "    y_train_tensor=torch.as_tensor(y_train).long()\n",
    "\n",
    "    X_val_tensor=torch.as_tensor(X_val).float()\n",
    "    y_val_tensor=torch.as_tensor(y_val).long()\n",
    "\n",
    "    train_dataset=TensorDataset(X_train_tensor,y_train_tensor)\n",
    "    val_dataset=TensorDataset(X_val_tensor,y_val_tensor)\n",
    "\n",
    "    train_loader=DataLoader(train_dataset,batch_size=256,shuffle=True)\n",
    "    val_loader=DataLoader(val_dataset,batch_size=256)\n",
    "        \n",
    "    \n",
    "    # Model config\n",
    "    learning_rate=0.01\n",
    "\n",
    "    torch.manual_seed(42)\n",
    "    model=nn.Sequential()\n",
    "    model.add_module('hidden1',nn.Linear(27,128))\n",
    "    model.add_module('activation1',nn.ReLU())\n",
    "    model.add_module('batchNorm1',nn.BatchNorm1d(128))\n",
    "    model.add_module('dropout1',nn.Dropout(p=0.2))\n",
    "\n",
    "    model.add_module('hidden2',nn.Linear(128,64))\n",
    "    model.add_module('activation2',nn.ReLU())\n",
    "    model.add_module('batchNorm2',nn.BatchNorm1d(64))\n",
    "    model.add_module('dropout2',nn.Dropout(p=0.2))\n",
    "\n",
    "    model.add_module('hidden3',nn.Linear(64,24))\n",
    "    model.add_module('activation3',nn.ReLU())\n",
    "    model.add_module('batchNorm3',nn.BatchNorm1d(24))\n",
    "    model.add_module('dropout3',nn.Dropout(p=0.2))\n",
    "\n",
    "    model.add_module('output',nn.Linear(24,7))\n",
    "    model.add_module('softmax',nn.Softmax(dim=1))\n",
    "\n",
    "    optimizer=optim.Adam(model.parameters(),lr=learning_rate)\n",
    "    loss_fn=nn.CrossEntropyLoss() \n",
    "\n",
    "    es_patience=25\n",
    "\n",
    "    # Model Training\n",
    "    n_epochs=250\n",
    "    ann=DNN(model,loss_fn,optimizer,es_patience)\n",
    "    ann.set_loaders(train_loader,val_loader)\n",
    "    ann.set_scheduler(patience=5,factor=0.3) #ann.set_scheduler(patience=5,factor=0.4)\n",
    "    ann.train(n_epochs)\n",
    "\n",
    "    '''---------------------------------------------------------------------------------------------'''\n",
    "    \n",
    "    y_pred_test=ann.predict(test).argmax(axis=1)\n",
    "    all_preds.append(y_pred_test)\n",
    "    acc_scores.append(accuracy_score(y_val,ann.predict(X_val).argmax(axis=1)))\n",
    "    total_losses.append(ann.val_losses)\n",
    "\n",
    "final_preds=get_mode_by_position(all_preds)\n",
    "print(f\"\\n\\nMean Accuracy: {np.mean(acc_scores)}\\nCustom score: {np.round((1.29-np.mean([np.mean(losses) for losses in total_losses]))*1e5,2)}\")\n",
    "    \n",
    "# create a submission dataframe\n",
    "prediction=pd.DataFrame({'id':submission['id'],'NObeyesdad':final_preds})\n",
    "submission[\"NObeyesdad\"]=prediction[\"NObeyesdad\"].map({0:'Insufficient_Weight',\n",
    "                                                        1:'Normal_Weight',\n",
    "                                                        2:'Overweight_Level_I',\n",
    "                                                        3:'Overweight_Level_II',\n",
    "                                                        4:'Obesity_Type_I',\n",
    "                                                        5:'Obesity_Type_II',\n",
    "                                                        6:'Obesity_Type_III'})\n",
    "\n",
    "submission.to_csv('submission_torch_mode3.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1591.15}"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{np.round((1.29-np.mean([np.mean(losses) for losses in total_losses]))*1e5,2)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>NObeyesdad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20758</td>\n",
       "      <td>Obesity_Type_II</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20759</td>\n",
       "      <td>Overweight_Level_I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20760</td>\n",
       "      <td>Obesity_Type_III</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20761</td>\n",
       "      <td>Obesity_Type_I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20762</td>\n",
       "      <td>Obesity_Type_III</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id          NObeyesdad\n",
       "0  20758     Obesity_Type_II\n",
       "1  20759  Overweight_Level_I\n",
       "2  20760    Obesity_Type_III\n",
       "3  20761      Obesity_Type_I\n",
       "4  20762    Obesity_Type_III"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
